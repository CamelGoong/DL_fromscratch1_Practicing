import numpy as np

# 오차제곱합 구현
def sum_squares_error(y, t):
  return 0.5 * np.sum((y-t)**2)

# 정답은 2레이블
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# 2레이블일 확률이 가장 높다고 추정
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]

sum_squares_error(np.array(y), np.array(t))

# 7레이블일 확률이 가장 높다고 추정
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]

sum_squares_error(np.array(y), np.array(t))

# 교차 엔트로피 오차 함수 구현
import numpy as np

def cross_entropy_error(y, t):
  delta = 1e-7 # 아주 작은 값인 delta를 np.log함수에 넣어서, 마이너스 무한대가 되어서 계산이 불가능한 상황을 막기위함.

  return -np.sum(t*np.log(y + delta))

# 정답 레이블은 2로 고정
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# 예측한 레이블이 2인 경우
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
print(cross_entropy_error(np.array(y), np.array(t)))

# 예측한 레이블이 2가 아닌 경우
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1,0.0, 0.6, 0.0, 0.0]
print(cross_entropy_error(np.array(y), np.array(t)))

from google.colab import files
src = list(files.upload().values())[0]
open('mnist.py','wb').write(src)
import mnist

from mnist import load_mnist

(x_train, t_train), (x_test, t_test) = \
  load_mnist(normalize = True, one_hot_label = True)
  
  # np.random.choice() 써서 미니배치 학습으로 사용할 무작위 10개 인덱스 배열 추출

train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)

x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]

def cross_entropy_error(y, t):
  if y.ndim == 1:
    t = t.reshape(1, t.size)
    y = y.reshape(1, y.size)

  batch_size = y.shape[0]
  return -np.sum(t*np.log(y + 1e-7)) / batch_size

# 나쁜 미분함수 구현
def numerical_diff(f,x):
  h = 10e-50 # 아주 작은 h 값을 h에 할당
  return (f(x+h) - f(x)) / h

# 개선점 2가지를 추가한 미분함수 구현
def numerical_diff(f,x):
  h = 1e-4
  return (f(x+h) - f(x-h)) / 2*h
  
# y = 0.01x^2 + 0.1x
def function_1(x):
  return 0.01*x**2 + 0.1*x

# 그래프 그리기
import matplotlib.pylab as plt

x = np.arange(0.0, 2.0, 0.1) # 0.0~2.0 범위에서 0.1씩 증가
y = function_1(x)

plt.xlabel("x")
plt.ylabel("f(x)")
plt.plot(x,y)
plt.show()

# 미분값 계산
print(numerical_diff(function_1, 5))
print(numerical_diff(function_1, 10))

# f(x0, x1) = x0^2 + x1^2 구현
def function_2(x): # Input의 배열 형태로 들어온다고 가정.
  return x[0]**2 + x[1]**2
  
  # 문제1: x0 = 3, x1 = 4일때, x0에 대한 편미분 구하기

def function_tmp1(x0):
  return x0*x0 + 4.0**2.0 # x0이 아닌 항인 x1은 그냥 상수로 간주해서 값을 대입해버림!

numerical_diff(function_tmp1, 3.0)

# 문제2: x0 = 3, x1 = 4일때, x1에 대한 편미분 구하기

def function_tmp2(x1):
  return x1*x1 + 3.0**2.0

numerical_diff(function_tmp2, 0.4)

def numerical_gradient(f, x):
  h = 1e-4 # 0.0001
  grad = np.zeros_like(x) # 기울기인 grad를 Input 배열인 X의 크기에 맞춰서 0으로 초기화

  for idx in range(x.size): # size는 전체 원소의 갯수
    tmp_val = x[idx] # 한 index의 원소씩 가져오면서 처리

    # f(x+h)의 계산
    x[idx] = tmp_val + h
    fxh1 = f(x)

    # f(x-h)의 계산
    x[idx] = tmp_val - h
    fxh2 = f(x)

    grad[idx] = (fxh1 - fxh2) / 2*h # 기울기 배열에 인덱스별로 순차적으로 입력
    x[idx] = tmp_val # x 배열에 각 인덱스에 원래 초기값 복원

  return grad

# 기울기 계산 Test
print(numerical_gradient(function_2, np.array([3.0, 4.0])))
print(numerical_gradient(function_2, np.array([0.0, 2.0])))
print(numerical_gradient(function_2, np.array([3.0, 0.0])))

def gradient_descent(f, init_x, lr = 0.01, step_number = 100):
  x = init_x

  for i in range(step_number):
    grad = numerical_gradient(f, x)
    x -= lr*grad
  return x
  
  # 문제: 경사하강법으로 f(x0,x1) = x0^2 + x1^2의 최솟값 구하기
def function_2(x):
  return x[0]**2 + x[1]**2

init_x = np.array([-3.0, 4.0])

gradient_descent(function_2, init_x = init_x, lr=0.1, step_number = 100)

# 학습률을 변화시키면서 값을 Test

# 학습률이 너무 큰 경우: lr = 10.0
init_x = np.array([-3.0, 4.0])
print(gradient_descent(function_2, init_x = init_x, lr = 10.0, step_number = 100))


# 학습률이 너무 작은 경우: lr = 1e-10
init_x = np.array([-3.0, 4.0])
print(gradient_descent(function_2, init_x = init_x, lr = 1e-10, step_number = 100))

# 이하내용 common function 코드 import 한 것.
# coding: utf-8
import numpy as np


def identity_function(x):
    return x


def step_function(x):
    return np.array(x > 0, dtype=np.int)


def sigmoid(x):
    return 1 / (1 + np.exp(-x))    


def sigmoid_grad(x):
    return (1.0 - sigmoid(x)) * sigmoid(x)
    

def relu(x):
    return np.maximum(0, x)


def relu_grad(x):
    grad = np.zeros(x)
    grad[x>=0] = 1
    return grad
    

def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T 

    x = x - np.max(x) # 오버플로 대책
    return np.exp(x) / np.sum(np.exp(x))


def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)


def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환
    if t.size == y.size:
        t = t.argmax(axis=1)
             
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size


def softmax_loss(X, t):
    y = softmax(X)
    return cross_entropy_error(y, t)

# 이하내용 gradient.py import한 것
# coding: utf-8
import numpy as np

def _numerical_gradient_1d(f, x):
    h = 1e-4 # 0.0001
    grad = np.zeros_like(x)
    
    for idx in range(x.size):
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + h
        fxh1 = f(x) # f(x+h)
        
        x[idx] = tmp_val - h 
        fxh2 = f(x) # f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2*h)
        
        x[idx] = tmp_val # 값 복원
        
    return grad


def numerical_gradient_2d(f, X):
    if X.ndim == 1:
        return _numerical_gradient_1d(f, X)
    else:
        grad = np.zeros_like(X)
        
        for idx, x in enumerate(X):
            grad[idx] = _numerical_gradient_1d(f, x)
        
        return grad


def numerical_gradient(f, x):
    h = 1e-4 # 0.0001
    grad = np.zeros_like(x)
    
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + h
        fxh1 = f(x) # f(x+h)
        
        x[idx] = tmp_val - h 
        fxh2 = f(x) # f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2*h)
        
        x[idx] = tmp_val # 값 복원
        it.iternext()   
        
    return grad

# 이하내용 SimpleNet import하고 일부 수정한 것
# coding: utf-8
class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2,3) # 정규분포로 초기화

    def predict(self, x):
        return np.dot(x, self.W)

    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)

net = simpleNet()
print(net.W) # 정규분포로 초기화된 가중치 매개변수

x = np.array([0.6, 0.9])
p = net.predict(x)
print(p)

np.argmax(p) # 최댓값의 인덱스

t = np.array([0, 0, 1])
net.loss(x, t)

def f(W):
  return net.loss(x, t)

dW = numerical_gradient(f, net.W)
print(dW)

        return loss

