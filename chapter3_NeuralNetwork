import numpy as np

def step_function(x):
  y = x > 0 # Input값이 넘파이 배열일 수 있으므로, 이에 대해 부등호 연산을 하면, y에는 배열의 각 원소에 대한 부등호 연산의 Boolean 값이 들어가게 됨.
  return y.astype(np.int) # 넘파일 배열 형식 -> int 형식으로 변환

step_function(np.array([-1.0, 1.0, 2.0]))

import numpy as np
import matplotlib.pyplot as plt

def step_fuction(x):
  return np.array(x>0, dtype = np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)

plt.plot(x, y)
plt.ylim(-0.1, 1.1) # y축의 범위 지정
plt.show()

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

x = np.array([-1.0, 1.0, 2.0])
sigmoid(x)

# 시그모이드 함수 그리기
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)

plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.title("Sigmoid")
plt.show()

def relu(x):
  return np.maximum(0, x) # 이렇게 함으로써, 0보다 작을 때는 그냥 0 / 0보다 클때는 그 값 x를 그대로 반환하는 relu 함수 구현가능
  
  import numpy as np
A = np.array([1, 2, 3, 4])
print(A)
print(A.shape)
print(np.ndim(A))

B = np.array([[1,2], [3,4], [5,6]])
print(B)
print(B.shape)
print(np.ndim(B))

A = np.array([[1,2], [3,4]])
A.shape

B = np.array([[5,6], [7,8]])
B.shape

np.dot(A, B)

A = np.array([[1, 2], [3, 4], [5, 6]])
A.shape
B = np.array([7, 8])
B.shape
np.dot(A,B)

X = np.array([1,2])
X.shape

W = np.array([[1, 3, 5], [2, 4, 6]])
W.shape

Y = np.dot(X, W)
print(Y)

import numpy as np

# 신경망 1층 구현
X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

A1 = np.dot(X, W1) + B1

Z1 = sigmoid(A1)

print(A1)
print(Z1)

# 신경망 2층 구현
W2 = np.array([[0.1,0.4], [0.2,0.5], [0.3,0.6]])
B2 = np.array([0.1,0.2])

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)

# 신경망 3층 구현(2층 -> Ouptput Layer)
def identity_function(x):
  return x

W3 = np.array([[0.1,0.3], [0.2, 0.4]])
B3 = np.array([0.1,0.2])
A3 = np.dot(Z2, W3) + B3

Y = identity_function(A3)
print(Y)

# init_network() 함수와 forward() 함수 정의

def init_network():
  network = {} # 딕셔너리 형태로 구현
  network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
  network['b1'] = np.array([0.1, 0.2, 0.3])
  network['W2'] = np.array([[0.1,0.4], [0.2,0.5], [0.3,0.6]])
  network['b2'] = np.array([0.1,0.2])
  network['W3'] = np.array([[0.1,0.3], [0.2, 0.4]])
  network['b3'] = np.array([0.1,0.2])

  return network

def forward(network, x):
  W1, W2, W3 = network['W1'], network['W2'], network['W3']
  b1, b2, b3 = network['b1'], network['b2'], network['b3']

  a1 = np.dot(x, W1) + b1
  z1 = sigmoid(a1)
  a2 = np.dot(z1, W2) + b2
  z2 = sigmoid(a2)
  a3 = np.dot(z2, W3) + b3
  y = identity_function(a3)

  return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)

print(y)

a = np.array([0.3, 2.9, 4.0])

exp_a = np.exp(a) # 지수함수
print(exp_a)

sum_exp_a = np.sum(exp_a) # 지수함수의 합
print(sum_exp_a)

y = exp_a / sum_exp_a
print(y)

# 소프트맥스 함수 구현
def softmax(a):
  exp_a = np.exp(a)
  sum_exp_a = np.sum(exp_a)
  y = exp_a / sum_exp_a

  return y

# 소프트맥스 함수 구현시 주의점
a = np.array([1010, 1000, 990])
np.exp(a) / np.sum(np.exp(a))

c = np.max(a) # a에서 최댓값을 상수c로 지정
a - c

np.exp(a-c), np.sum(np.exp(a-c))

# 개선된 소프트맥스 함수 구현
def softmax(a):
  c = np.max(a)
  exp_a = np.exp(a-c)
  exp_a_sum = np.sum(exp_a)
  y = exp_a / exp_a_sum

  return y

# 소프트맥스 함수 Test
a = np.array([0.3, 2.9, 4.0])
y = softmax(a)
print(y)
print(np.sum(y))

from google.colab import drive
drive.mount('/gdrive')

import sys, os
sys.path.append(os.pardir)

# 구글 drive에서 .py파일 가져오기!
from google.colab import files
src = list(files.upload().values())[0]
open('mnist.py','wb').write(src)
import mnist

from mnist import load_mnist

# MNIST 데이터셋 읽기
(X_train, t_train), (X_test, t_test) = load_mnist(flatten = True, normalize = False)

import sys, os
sys.path.append(os.pardir)
import numpy as np
from PIL import Image

def img_show(img):
  pil_img = Image.fromarray(np.uint8(img))
  pil_img.show()

# (x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False)
# 위에서 데이터 한번 읽어온 거라서 주석처리

img = X_train[0]
label = t_train[0]
print(label)

print(img.shape) # flatten으로 numpy 1차원 배열 상태의 image
img = img.reshape(28, 28) # 원래 이미지 모양으로 reshape
print(img.shape)

img_show(img)

import cv2
from google.colab.patches import cv2_imshow
cv2_imshow(img)      

import numpy as np

def get_data():
  (x_train, t_train), (x_test, t_test) = \
    load_mnist(flatten = True, normalize = False)
  
  return x_test, t_test

def init_network():
    network = np.load('sample_weight.pkl', allow_pickle = True)
      
        
    return network

def predict(network, x):
  
  W1, W2, W3 = network['W1'], network['W2'], network['W3']
  b1, b2, b3 = network['b1'], network['b2'], network['b3']

  a1 = np.dot(x, W1) + b1
  z1 = sigmoid(a1)
  a2 = np.dot(z1, W2) + b2
  z2 = sigmoid(a2)
  a3 = np.dot(z2, W3) + b3
  y = identity_function(a3)

  return y


x, t = get_data()
network = init_network()

accuracy_cnt = 0 # 정확도 초기화
for i in range(len(x)):
  y = predict(network, x[i])
  p = np.argmax(y) # 확률이 가장 높은 원소(분류)의 인덱스를 얻기
  if p == t[i]:
    accuracy_cnt += 1

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

x, _ = get_data()
network = init_network()
W1, W2, W3 = network['W1'], network['W2'], network['W3']
print(x.shape)
print(x[0].shape)
print(W1.shape)
print(W2.shape)
print(W3.shape)

x, t = get_data()
network = init_network()

batch_size = 100 # 배치크기 설정
accuracy_cnt = 0

for i in range(0, len(x), batch_size):
  x_batch = x[i:i+batch_size]
  y_batch = predict(network, x_batch)
  p = np.argmax(y_batch, axis =1)
  accuracy_cnt += np.sum(p == t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

# argmax() 에서 axis = 1 인수에 대한 이해
x = np.array([[0.1, 0.8, 0.1], [0.3, 0.1, 0.6], [0.2, 0.5, 0.3], [0.8, 0.1, 0.1]])
y = np.argmax(x, axis = 1)
z = np.argmax(x, axis = 0)
print(y)
print(z)

# np.sum()을 이용해서 예측 레이블 = 실제 레이블의 갯수를 나오게 하는 원리

y = np.array([1, 2, 1, 0])
t = np.array([1, 2, 0, 0])

print(y == t)
print(np.sum(y == t))

